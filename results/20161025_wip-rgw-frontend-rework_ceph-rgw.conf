; generated by vstart.sh on pon, 5 pa≈∫ 2015, 16:24:32 CEST
[global]
        fsid = 39e58ef3-c1d3-40f4-b535-68261a515aab
        osd pg bits = 3
        osd pgp bits = 5  ; (invalid, but ceph should cope!)
        osd crush chooseleaf type = 0
        osd pool default min size = 1
        osd failsafe full ratio = .99
        mon osd full ratio = .99
        mon data avail warn = 10
        mon data avail crit = 1
        erasure code dir = .libs
        osd pool default erasure code profile = plugin=jerasure technique=reed_sol_van k=2 m=1 ruleset-failure-domain=osd
        rgw frontends = fastcgi, civetweb port=8002, prefix=/swift, asio port=8000
        ;rgw frontends = fastcgi, civetweb port=8002 prefix=/swift, asio port=8000
        rgw dns name = localhost
        filestore fd cache size = 32
        run dir = /work/ceph-2/src/out
        enable experimental unrecoverable data corrupting features = *
        auth supported = cephx

[client]
        keyring = /work/ceph-2/src/keyring
        log file = /work/ceph-2/src/out/$name.$pid.log
        admin socket = /work/ceph-2/src/out/$name.$pid.asok

[client.radosgw.gateway]
        keyring = ./ceph.client.radosgw.keyring
        rgw keystone url = http://localhost:35357
        rgw keystone admin token = ADMIN
        rgw keystone accepted roles = admin,Member
        rgw s3 auth use keystone = false

[mds]

	log file = /work/ceph-2/src/out/$name.log
        admin socket = /work/ceph-2/src/out/$name.asok
	chdir = ""
	pid file = /work/ceph-2/src/out/$name.pid
        heartbeat file = /work/ceph-2/src/out/$name.heartbeat


        debug ms = 1
        mds debug frag = true
        mds debug auth pins = true
        mds debug subtrees = true
        mds data = /work/ceph-2/src/dev/mds.$id

[osd]

	log file = /work/ceph-2/src/out/$name.log
        admin socket = /work/ceph-2/src/out/$name.asok
	chdir = ""
	pid file = /work/ceph-2/src/out/$name.pid
        heartbeat file = /work/ceph-2/src/out/$name.heartbeat

        osd data = /work/ceph-2/src/dev/osd$id
        osd journal = /work/ceph-2/src/dev/osd$id.journal
        osd journal size = 100
        osd class tmp = out
        osd class dir = .libs
        osd scrub load threshold = 5.0
        osd debug op order = true
        filestore wbthrottle xfs ios start flusher = 10
        filestore wbthrottle xfs ios hard limit = 20
        filestore wbthrottle xfs inodes hard limit = 30
        filestore wbthrottle btrfs ios start flusher = 10
        filestore wbthrottle btrfs ios hard limit = 20
        filestore wbthrottle btrfs inodes hard limit = 30

        debug ms = 1


[mon]
        mon pg warn min per osd = 3
        mon osd allow primary affinity = true
        mon reweight min pgs per osd = 4
        mon osd prime pg temp = true
        crushtool = ./crushtool

	log file = /work/ceph-2/src/out/$name.log
        admin socket = /work/ceph-2/src/out/$name.asok
	chdir = ""
	pid file = /work/ceph-2/src/out/$name.pid
        heartbeat file = /work/ceph-2/src/out/$name.heartbeat


	debug mon = 10
        debug ms = 1

        mon cluster log file = /work/ceph-2/src/out/cluster.mon.$id.log
[global]

[mon.a]
        host = adam-VirtualBox
        mon data = /work/ceph-2/src/dev/mon.a
        mon addr = 127.0.0.1:6789
[mon.b]
        host = adam-VirtualBox
        mon data = /work/ceph-2/src/dev/mon.b
        mon addr = 127.0.0.1:6790
[mon.c]
        host = adam-VirtualBox
        mon data = /work/ceph-2/src/dev/mon.c
        mon addr = 127.0.0.1:6791
[osd.0]
        host = adam-VirtualBox
[osd.1]
        host = adam-VirtualBox
[osd.2]
        host = adam-VirtualBox
[mds.a]
        host = adam-VirtualBox
[mds.b]
        host = adam-VirtualBox
[mds.c]
        host = adam-VirtualBox
